### **ADR-001: Selectable Schedule-Free Optimizers**

**ステータス:** 承認 (Accepted)

**日付:** 2025-08-05

#### **コンテキスト (Context)**

現在の学習プロセスでは、`schedulefree` ライブラリの `AdamWScheduleFree` オプティマイザが標準として使用されています。これは、学習率スケジューラの手動調整を不要にし、安定した学習を実現するためのものです。

しかし、参考文献[1]およびその基礎となる論文[2]において、`RAdam` オプティマイザと `ScheduleFree` を組み合わせた `RAdamScheduleFree` が、特に学習初期段階における分散のばらつきを抑制し、`AdamW` ベースのアプローチよりも安定した収束をもたらす可能性が示唆されています。

現在の実装は `AdamWScheduleFree` に固定されており、ユーザーがこの新しいアプローチの恩恵を受けられるかどうかの検証や、モデルに合わせた最適なオプティマイザを選択する柔軟性が欠けていました。

#### **決定 (Decision)**

学習の安定性と最終的なモデル品質を向上させる選択肢をユーザーに提供するため、以下の変更を導入します。

1.  **設定の追加:** 学習設定ファイル (`config.json`) の `train` オブジェクト内に、新しいキー `"optimizer"` を導入します。
    *   このキーは `"AdamW"` または `"RAdam"` の文字列を受け付けます。

2.  **動的切り替えの実装:** 学習スクリプト (`train_ms_jp_extra.py`) を修正し、`"optimizer"` キーの値を読み取って使用するオプティマイザを動的に決定します。
    *   `"RAdam"` が指定された場合、`RAdamScheduleFree` を使用します。
    *   `"AdamW"` が指定された場合、またはキーが存在しない場合（後方互換性のため）、従来の `AdamWScheduleFree` を使用します。

3.  **デフォルト設定の変更:** テンプレートとなる `configs/config_jp_extra.json` では、デフォルト値を `"RAdam"` に設定し、新規プロジェクトが `RAdam` の恩恵を受けやすくします。

4.  **ログ出力:** 起動時に、現在どちらのオプティマイザが有効になっているかを明確にログ出力し、ユーザーが現在の設定を容易に確認できるようにします。

#### **結果 (Consequences)**

**ポジティブ:**

*   **柔軟性の向上:** ユーザーはコンフィグファイルを一行変更するだけで、2つの先進的なオプティマイザを簡単に切り替え、自身のデータセットやモデルに最適なものを選択できます。
*   **パフォーマンス向上の可能性:** `RAdam` の導入により、特に学習初期が不安定なモデルにおいて、より安定した収束と高いモデル品質を達成できる可能性があります。
*   **実験の容易化:** プロジェクト内でオプティマイザのA/Bテストを容易に行えるようになります。
*   **技術的先進性の維持:** プロジェクトが最適化手法の最新の研究動向に追従していることを保証します。

**ネガティブ:**

*   **複雑性の微増:** 学習スクリプト内にオプティマイザを切り替えるための条件分岐が追加され、コードの複雑性がわずかに増加します。
*   **設定項目の増加:** ユーザーが認識すべき設定ファイル内の項目が一つ増えます。

#### **引用文献 (References)**

[1] 全ての学習率スケジューリングを過去にするOptimizer - Zenn. (2024). Retrieved from [https://zenn.dev/dena/articles/6f04641801b387](https://zenn.dev/dena/articles/6f04641801b387)

[2] Anonymous, et al. "Schedule-Free Learning: A New Way to Train Deep Models." *arXiv preprint arXiv:2405.15682*, 2024.
